---
title: "Class 7: Machine Learning 1"
author: Austin Teel (A17293709)
format: html
toc: True
---

## Background
Today we will begin our exploration of some important machine learning methods, namely **clustering** and **dimensionality reduction**

Let's make up some input data for clustering where we know what the natural "clusters" are.

The function `rnorm()` can be useful here.


```{r}
hist(rnorm(5000, mean=10, 1))
```

> Q. Generate 30 random numbers centered at +3 and another 30 centered at -3

```{r}
tmp <- c(rnorm(30,mean=3),
         rnorm(30,-3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```


## K-means cluster

The main function in "base R" for K-means clustering is called `kmeans()`:

```{r}
km <- kmeans(x,centers=2)
km
```

> Q. What component of the results object details the cluster sizes?

```{r}
km$size
```

> Q. What component of the results object details the cluster centers?

```{r}
km$centers
```

> Q. What component of the results object details the cluster membership vector (i.e. our main result of which points lie in which cluster)?

```{r}
km$cluster
```

> Q. Plot our clustering results with points colored by cluster and also add the cluster centers as new points colored blue?

```{r}
plot(x, col=km$cluster)
points(km$centers, col="blue", pch=15)
```

> Q. run `kmeans` again and this time produce 4 clusters 9and call your result object `k4`) and make a results figure like above?


```{r}
k4 <- kmeans(x,centers=4)
k4
plot(x, col=k4$cluster)
points(k4$centers, col="blue", pch=15)
```

The metric
```{r}
km$tot.withinss
k4$tot.withinss
```

> Q. Let's try different number of K (centers) from 1 to 30 and see what the best result is?


```{r}
ans <- NULL
for(i in 1:30){
ans <- c(ans, kmeans(x, centers=i)$tot.withinss)
}
```

```{r}
plot(ans, type="o")
```

**Key-pont:** K-means will impose a clustering structure on your data even if it is not there - it will always give you the answer you asked for even if that answer is silly. With `tot.withinss` you are able to see that the higher your centers amount there are are the lower your value will be which means the spreads will be and the tighter the clusters will be.


## Hierarchical Clustering

The main function for Hierarchical Clustering is called `hclust()`. 

Unlike `kmeans()` (which does all of the work for you) you can't just pass `hclust()` our raw data input. It needs a "distance matrix" like the one returned from the `dist()` function.


```{r}
d <- dist(x)
hc <- hclust(d)
plot(hc)
```

To extract our cluster membership vector from a `hclust()` result object we have to "cut" our tree at a given height to yield seperate "groups"/"branches".


```{r}
plot(hc)
abline(h=8, col="red", lty=2)
```

To do this we use the `cutree()` function on our `hclust()` object:

```{r}
grps <- cutree(hc, h=8)
grps
```
```{r}
table(grps,
km$cluster)
```

## PCA of UK food data

Import the dataset of food consumption in the UK: 

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```


> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
nrow(x)
ncol(x)
#or
dim(x)
```

One solution to set the row names is to do it by hand...

```{r}
rownames(x) <- x[,1]
x
```

To remove the first column I can use the minus index trick

```{r}
x <- x[,-1]
x
```

A better way to do this is to set the row names of the first column with `read.csv()`

```{r}
x <- read.csv(url, row.names=1)
x
```


> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

## Spotting major differences and trends

Is difficult even in this wee 17D dataset...

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

### Pairs plot and heatmaps

```{r}
pairs(x, col=rainbow(nrow(x)), pch=16)
```

```{r}
library(pheatmap)

pheatmap( as.matrix(x) )

```


## PCA to the rescue

The main PCA function in "base R" is called `prcomp()`. This function wants the transpose of our food data as input (i.e. the foods as columns and the countries as rows).



```{r}
pca <- prcomp(t(x))
```

```{r}
summary(pca)
```

```{r}
attributes(pca)
```

To make one of the main PCA results figures we turn to `pca$x` the scores along our new PCs. This is called "PC plot" or "score plot: or "ordination plot" ...

```{r}
pca$x
```

```{r}
my_cols <- c("orange","red","blue","green")
```


```{r}
library(ggplot2)

ggplot(pca$x)+
  aes(PC1,PC2)+
  geom_point(col=my_cols)
```

The second major resuly figure is called a "loadings plot" or "variable contributions plot" or "weight plot".

```{r}
ggplot(pca$rotation)+
  aes(PC1, rownames(pca$rotation)) +
  geom_col()
```











